{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R40E6c28bYso",
        "outputId": "fe5ec6db-3c11-4f73-9432-fb9fd2fb63ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "vocab_size = 9\n",
            "MASK_ID    = 8\n",
            "[Step  300] loss = 0.4220\n",
            "  [t=T noise] clean :  We love NLP .\n",
            "             noised:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred  :  I love NLP .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    I love NLP .\n",
            "====================================\n",
            "[Step  600] loss = 0.3681\n",
            "  [t=T noise] clean :  I love AI .\n",
            "             noised:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred  :  We love NLP .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    We love NLP .\n",
            "====================================\n",
            "[Step  900] loss = 0.3224\n",
            "  [t=T noise] clean :  I love AI .\n",
            "             noised:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred  :  You love NLP .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    You love NLP .\n",
            "====================================\n",
            "[Step 1200] loss = 0.2537\n",
            "  [t=T noise] clean :  We love NLP .\n",
            "             noised:  We love [MASK] [MASK]\n",
            "             pred  :  We love NLP .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    I love NLP .\n",
            "====================================\n",
            "[Step 1500] loss = 0.2794\n",
            "  [t=T noise] clean :  We love AI .\n",
            "             noised:  [MASK] [MASK] [MASK] .\n",
            "             pred  :  You love AI .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    You love AI .\n",
            "====================================\n",
            "[Step 1800] loss = 0.2228\n",
            "  [t=T noise] clean :  You love AI .\n",
            "             noised:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred  :  We love AI .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    We love AI .\n",
            "====================================\n",
            "[Step 2100] loss = 0.2768\n",
            "  [t=T noise] clean :  You love AI .\n",
            "             noised:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred  :  You love AI .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    You love AI .\n",
            "====================================\n",
            "[Step 2400] loss = 0.3193\n",
            "  [t=T noise] clean :  You love AI .\n",
            "             noised:  You [MASK] AI [MASK]\n",
            "             pred  :  You love AI .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    I love AI .\n",
            "====================================\n",
            "[Step 2700] loss = 0.2261\n",
            "  [t=T noise] clean :  We love NLP .\n",
            "             noised:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred  :  You love AI .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    You love AI .\n",
            "====================================\n",
            "[Step 3000] loss = 0.3124\n",
            "  [t=T noise] clean :  I love AI .\n",
            "             noised:  [MASK] love AI [MASK]\n",
            "             pred  :  I love AI .\n",
            "  ----------------------------\n",
            "  [FULL MASK]   x_t:  [MASK] [MASK] [MASK] [MASK]\n",
            "             pred:    I love NLP .\n",
            "====================================\n",
            "\n",
            "===== Generation from FULL [MASK] tokens =====\n",
            "Sample 0:\n",
            "  x_T([MASK]): [MASK] [MASK] [MASK] [MASK]\n",
            "  predicted x0: I love NLP .\n",
            "----------------------------------------\n",
            "Sample 1:\n",
            "  x_T([MASK]): [MASK] [MASK] [MASK] [MASK]\n",
            "  predicted x0: I love NLP .\n",
            "----------------------------------------\n",
            "Sample 2:\n",
            "  x_T([MASK]): [MASK] [MASK] [MASK] [MASK]\n",
            "  predicted x0: I love NLP .\n",
            "----------------------------------------\n",
            "Sample 3:\n",
            "  x_T([MASK]): [MASK] [MASK] [MASK] [MASK]\n",
            "  predicted x0: I love NLP .\n",
            "----------------------------------------\n",
            "Sample 4:\n",
            "  x_T([MASK]): [MASK] [MASK] [MASK] [MASK]\n",
            "  predicted x0: I love NLP .\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# =========================\n",
        "# 0. Basic Setup & Data\n",
        "# =========================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Toy sentence dataset: each sequence has fixed length 4\n",
        "sentences = [\n",
        "    [\"I\",   \"love\", \"AI\",  \".\"],\n",
        "    [\"You\", \"love\", \"AI\",  \".\"],\n",
        "    [\"We\",  \"love\", \"AI\",  \".\"],\n",
        "    [\"I\",   \"love\", \"NLP\", \".\"],\n",
        "    [\"You\", \"love\", \"NLP\", \".\"],\n",
        "    [\"We\",  \"love\", \"NLP\", \".\"],\n",
        "]\n",
        "\n",
        "seq_len = 4\n",
        "\n",
        "# Explicitly include [MASK] token in the vocabulary\n",
        "vocab = [\"<pad>\", \"I\", \"You\", \"We\", \"love\", \"AI\", \"NLP\", \".\", \"[MASK]\"]\n",
        "token2id = {tok: i for i, tok in enumerate(vocab)}\n",
        "id2token = {i: tok for tok, i in token2id.items()}\n",
        "MASK_ID = token2id[\"[MASK]\"]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(\"vocab_size =\", vocab_size)\n",
        "print(\"MASK_ID    =\", MASK_ID)\n",
        "\n",
        "def encode_sentence(tokens):\n",
        "    assert len(tokens) == seq_len\n",
        "    return torch.tensor([token2id[t] for t in tokens], dtype=torch.long)\n",
        "\n",
        "data_x0 = torch.stack([encode_sentence(s) for s in sentences]).to(device)  # (N, L)\n",
        "num_samples = data_x0.size(0)\n",
        "\n",
        "# =========================\n",
        "# 1. Discrete Forward Process q(x_t | x_0)\n",
        "#    SEDD-style mask diffusion\n",
        "# =========================\n",
        "\n",
        "T = 5  # Total diffusion steps\n",
        "\n",
        "# Define per-step betas and compute alpha_bar_t\n",
        "betas = torch.linspace(0.1, 0.4, T).to(device)\n",
        "alphas = 1.0 - betas\n",
        "alpha_bars = torch.cumprod(alphas, dim=0)   # (T,)\n",
        "\n",
        "def q_sample_mask(x0, t):\n",
        "    \"\"\"\n",
        "    Simplified SEDD-style forward process:\n",
        "    For each position:\n",
        "      - With probability alpha_bar_t, keep x0\n",
        "      - With probability (1 - alpha_bar_t), replace with MASK\n",
        "\n",
        "    x0: (B, L)\n",
        "    t : (B,) in [1..T]\n",
        "    Returns x_t: (B, L)\n",
        "    \"\"\"\n",
        "    B, L = x0.shape\n",
        "\n",
        "    # Look up corresponding alpha_bar_t\n",
        "    alpha_bar_t = alpha_bars[(t - 1).long()]   # (B,)\n",
        "    alpha_bar_t = alpha_bar_t.view(B, 1)       # (B,1)\n",
        "\n",
        "    # Sample whether each token is kept\n",
        "    u = torch.rand(B, L, device=device)        # (B,L)\n",
        "    keep = (u < alpha_bar_t)                   # True -> keep x0\n",
        "\n",
        "    xt = torch.where(\n",
        "        keep,\n",
        "        x0,\n",
        "        torch.full_like(x0, MASK_ID)\n",
        "    )\n",
        "    return xt\n",
        "\n",
        "# =========================\n",
        "# 2. Simple SEDD Model\n",
        "#    Input:  x_t, t\n",
        "#    Output: logits over x_0\n",
        "# =========================\n",
        "\n",
        "class SimpleSEDDModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, T, seq_len):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb   = nn.Embedding(seq_len, d_model)\n",
        "        self.time_emb  = nn.Embedding(T + 1, d_model)  # t ∈ [1..T], index 0 unused\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model, vocab_size),\n",
        "        )\n",
        "\n",
        "        self.register_buffer(\"positions\", torch.arange(seq_len).long())\n",
        "\n",
        "    def forward(self, x_t, t):\n",
        "        \"\"\"\n",
        "        x_t: (B, L)\n",
        "        t  : (B,)\n",
        "        Returns logits: (B, L, V)\n",
        "        \"\"\"\n",
        "        B, L = x_t.shape\n",
        "\n",
        "        tok_emb = self.token_emb(x_t)                       # (B,L,D)\n",
        "        pos_emb = self.pos_emb(self.positions)[None, :, :]  # (1,L,D)\n",
        "        time_emb = self.time_emb(t).unsqueeze(1)            # (B,1,D)\n",
        "\n",
        "        h = tok_emb + pos_emb + time_emb                    # (B,L,D)\n",
        "        logits = self.mlp(h)                                # (B,L,V)\n",
        "        return logits\n",
        "\n",
        "model = SimpleSEDDModel(vocab_size=vocab_size, d_model=64, T=T, seq_len=seq_len).to(device)\n",
        "\n",
        "# =========================\n",
        "# 3. Score Entropy Loss\n",
        "#    Cross-entropy under a delta posterior\n",
        "# =========================\n",
        "\n",
        "def score_entropy_loss(logits, x0):\n",
        "    \"\"\"\n",
        "    logits: (B,L,V)\n",
        "    x0:     (B,L)\n",
        "\n",
        "    Loss = -E[ log p_theta(x0 | x_t, t) ]\n",
        "    \"\"\"\n",
        "    B, L, V = logits.shape\n",
        "    return F.cross_entropy(\n",
        "        logits.view(B * L, V),\n",
        "        x0.view(B * L),\n",
        "    )\n",
        "\n",
        "# =========================\n",
        "# 4. Training Loop\n",
        "#    Explicitly includes \"full MASK\" samples\n",
        "# =========================\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_steps = 3000\n",
        "batch_size = 8\n",
        "\n",
        "# Each sample has 30% chance of being replaced with full MASK at t = T\n",
        "full_mask_prob = 0.3\n",
        "\n",
        "def sample_batch():\n",
        "    idx = torch.randint(0, num_samples, (batch_size,), device=device)\n",
        "    x0 = data_x0[idx]                           # (B,L)\n",
        "\n",
        "    # Sample t ~ Uniform{1..T}\n",
        "    t = torch.randint(1, T + 1, (batch_size,), device=device)\n",
        "\n",
        "    # Forward diffusion to get masked inputs\n",
        "    x_t = q_sample_mask(x0, t)                  # (B,L)\n",
        "\n",
        "    # Explicitly include some \"full MASK at t=T\" samples\n",
        "    hard_mask = torch.rand(batch_size, device=device) < full_mask_prob\n",
        "    if hard_mask.any():\n",
        "        x_t[hard_mask] = MASK_ID\n",
        "        t[hard_mask]   = T\n",
        "\n",
        "    return x0, x_t, t\n",
        "\n",
        "def decode(x):\n",
        "    return \" \".join(id2token[int(i)] for i in x)\n",
        "\n",
        "for step in range(1, num_steps + 1):\n",
        "    model.train()\n",
        "    x0, x_t, t = sample_batch()\n",
        "    logits = model(x_t, t)\n",
        "    loss = score_entropy_loss(logits, x0)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 300 == 0:\n",
        "        print(f\"[Step {step:4d}] loss = {loss.item():.4f}\")\n",
        "\n",
        "        # Check whether model can recover from strong noise (t = T)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            demo_idx = random.randrange(num_samples)\n",
        "            clean = data_x0[demo_idx:demo_idx+1]       # (1,L)\n",
        "            t_demo = torch.tensor([T], device=device)\n",
        "            noised = q_sample_mask(clean, t_demo)      # (1,L)\n",
        "            logits_demo = model(noised, t_demo)\n",
        "            pred = logits_demo.argmax(dim=-1)\n",
        "\n",
        "            print(\"  [t=T noise] clean : \", decode(clean[0]))\n",
        "            print(\"             noised: \", decode(noised[0]))\n",
        "            print(\"             pred  : \", decode(pred[0]))\n",
        "            print(\"  ----------------------------\")\n",
        "\n",
        "            # Check \"full MASK → reconstruction\"\n",
        "            full_mask = torch.full_like(clean, MASK_ID)\n",
        "            logits_mask = model(full_mask, t_demo)\n",
        "            pred_mask = logits_mask.argmax(dim=-1)\n",
        "            print(\"  [FULL MASK]   x_t: \", decode(full_mask[0]))\n",
        "            print(\"             pred:   \", decode(pred_mask[0]))\n",
        "            print(\"====================================\")\n",
        "\n",
        "# =========================\n",
        "# 5. Generation from Full MASK (Testing Function)\n",
        "# =========================\n",
        "\n",
        "def generate_from_full_mask(n=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_mask = torch.full((n, seq_len), MASK_ID, device=device, dtype=torch.long)\n",
        "        t = torch.full((n,), T, device=device, dtype=torch.long)\n",
        "\n",
        "        logits = model(x_mask, t)\n",
        "        pred = logits.argmax(dim=-1)\n",
        "\n",
        "        for i in range(n):\n",
        "            print(f\"Sample {i}:\")\n",
        "            print(\"  x_T([MASK]):\", decode(x_mask[i]))\n",
        "            print(\"  predicted x0:\", decode(pred[i]))\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n===== Generation from FULL [MASK] tokens =====\")\n",
        "generate_from_full_mask(n=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "em2DjJqqbsH3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}