{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsJvlec5YyoW",
        "outputId": "5d1ec9fc-a814-4986-d460-fa798dd2fedf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "[Step  200] loss = 0.3905\n",
            "  clean:  We love NLP .\n",
            "  noised: We AI love You\n",
            "  pred:   We love NLP .\n",
            "----------------------------------------\n",
            "[Step  400] loss = 0.4113\n",
            "  clean:  We love NLP .\n",
            "  noised: We love You .\n",
            "  pred:   We love NLP .\n",
            "----------------------------------------\n",
            "[Step  600] loss = 0.3782\n",
            "  clean:  You love NLP .\n",
            "  noised: You . NLP .\n",
            "  pred:   You love NLP .\n",
            "----------------------------------------\n",
            "[Step  800] loss = 0.1961\n",
            "  clean:  I love AI .\n",
            "  noised: I love AI You\n",
            "  pred:   I love AI .\n",
            "----------------------------------------\n",
            "[Step 1000] loss = 0.2742\n",
            "  clean:  You love NLP .\n",
            "  noised: <pad> love AI love\n",
            "  pred:   I love AI .\n",
            "----------------------------------------\n",
            "[Step 1200] loss = 0.1769\n",
            "  clean:  I love NLP .\n",
            "  noised: We <pad> I .\n",
            "  pred:   We love NLP .\n",
            "----------------------------------------\n",
            "[Step 1400] loss = 0.3846\n",
            "  clean:  I love AI .\n",
            "  noised: We love <pad> We\n",
            "  pred:   We love AI .\n",
            "----------------------------------------\n",
            "[Step 1600] loss = 0.2724\n",
            "  clean:  I love AI .\n",
            "  noised: . love love I\n",
            "  pred:   We love AI .\n",
            "----------------------------------------\n",
            "[Step 1800] loss = 0.1913\n",
            "  clean:  I love AI .\n",
            "  noised: love AI You .\n",
            "  pred:   I love NLP .\n",
            "----------------------------------------\n",
            "[Step 2000] loss = 0.2742\n",
            "  clean:  You love AI .\n",
            "  noised: love NLP AI love\n",
            "  pred:   We love AI .\n",
            "----------------------------------------\n",
            "\n",
            "===== Generation from pure noise (one-step denoise) =====\n",
            "Sample 0:\n",
            "  x_T   : You AI <pad> I\n",
            "  x0hat : You love NLP .\n",
            "----------------------------------------\n",
            "Sample 1:\n",
            "  x_T   : NLP <pad> You .\n",
            "  x0hat : We love NLP .\n",
            "----------------------------------------\n",
            "Sample 2:\n",
            "  x_T   : We . love love\n",
            "  x0hat : We love NLP .\n",
            "----------------------------------------\n",
            "Sample 3:\n",
            "  x_T   : AI love We We\n",
            "  x0hat : We love AI .\n",
            "----------------------------------------\n",
            "Sample 4:\n",
            "  x_T   : I love . We\n",
            "  x0hat : I love NLP .\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# ======================\n",
        "# 1. Basic Setup\n",
        "# ======================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# A tiny toy corpus: each sentence has fixed length 4\n",
        "sentences = [\n",
        "    [\"I\",   \"love\", \"AI\", \".\"],\n",
        "    [\"You\", \"love\", \"AI\", \".\"],\n",
        "    [\"We\",  \"love\", \"AI\", \".\"],\n",
        "    [\"I\",   \"love\", \"NLP\", \".\"],\n",
        "    [\"You\", \"love\", \"NLP\", \".\"],\n",
        "    [\"We\",  \"love\", \"NLP\", \".\"],\n",
        "]\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = [\"<pad>\", \"I\", \"You\", \"We\", \"love\", \"AI\", \"NLP\", \".\"]\n",
        "token2id = {tok: i for i, tok in enumerate(vocab)}\n",
        "id2token = {i: tok for tok, i in token2id.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "seq_len = 4\n",
        "\n",
        "def encode_sentence(tokens):\n",
        "    assert len(tokens) == seq_len\n",
        "    return torch.tensor([token2id[t] for t in tokens], dtype=torch.long)\n",
        "\n",
        "data_x0 = torch.stack([encode_sentence(s) for s in sentences])  # (N, L)\n",
        "data_x0 = data_x0.to(device)\n",
        "num_samples = data_x0.size(0)\n",
        "\n",
        "# =======================\n",
        "# 2. Discrete Forward Diffusion Process q(x_t | x_0)\n",
        "#    Simplified version: categorical noise (keep or random token)\n",
        "# =======================\n",
        "\n",
        "T = 5  # Number of diffusion steps (small for demonstration)\n",
        "betas = torch.linspace(0.1, 0.3, T).to(device)  # Noise strength per step\n",
        "alphas = 1.0 - betas\n",
        "alpha_bars = torch.cumprod(alphas, dim=0)  # Cumulative product of alphas\n",
        "\n",
        "def q_sample(x0, t):\n",
        "    \"\"\"\n",
        "    x0: (B, L) integer tokens\n",
        "    t:  (B,) integer timesteps in [1..T]\n",
        "    Returns x_t: (B, L)\n",
        "\n",
        "    Simplified discrete forward process:\n",
        "      q(x_t | x_0) = ᾱ_t * one_hot(x0) + (1 - ᾱ_t) * Uniform\n",
        "    \"\"\"\n",
        "    B, L = x0.shape\n",
        "    K = vocab_size\n",
        "\n",
        "    # Look up alpha_bar_t (t ranges 1..T, so subtract 1 for indexing)\n",
        "    alpha_bar = alpha_bars[t - 1]              # (B,)\n",
        "    alpha_bar = alpha_bar.view(B, 1, 1)        # (B,1,1)\n",
        "\n",
        "    # Start from uniform distribution over vocabulary\n",
        "    p = torch.full((B, L, K),\n",
        "                   fill_value=(1.0 / K),\n",
        "                   device=device)              # (B,L,K)\n",
        "\n",
        "    # Compute: p = (1 - alpha_bar) * Uniform + alpha_bar * OneHot(x0)\n",
        "    p = p * (1.0 - alpha_bar)                  # scale uniform part\n",
        "\n",
        "    idx = x0.unsqueeze(-1)                     # (B,L,1)\n",
        "    base = p.gather(-1, idx)                   # get current prob at x0 index\n",
        "    p.scatter_(-1, idx, base + alpha_bar)      # add alpha_bar into x0 index\n",
        "\n",
        "    # Sample x_t from categorical distribution\n",
        "    p_reshaped = p.view(-1, K)                  # (B*L, K)\n",
        "    xt_flat = torch.multinomial(p_reshaped, num_samples=1)  # (B*L,1)\n",
        "    xt = xt_flat.view(B, L)                     # (B,L)\n",
        "\n",
        "    return xt\n",
        "\n",
        "\n",
        "# =======================\n",
        "# 3. Simple SEDD-style Model\n",
        "#    Input:  x_t, t\n",
        "#    Output: Predicted distribution over x_0 at each position\n",
        "# =======================\n",
        "\n",
        "class SimpleSEDDModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, T, seq_len):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb   = nn.Embedding(seq_len, d_model)\n",
        "        self.time_emb  = nn.Embedding(T + 1, d_model)  # t in [1..T], index 0 unused\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model, vocab_size),\n",
        "        )\n",
        "\n",
        "        # Precompute positional indices 0..L-1\n",
        "        self.register_buffer(\"positions\", torch.arange(seq_len).long())\n",
        "\n",
        "    def forward(self, x_t, t):\n",
        "        \"\"\"\n",
        "        x_t: (B, L)\n",
        "        t:   (B,)\n",
        "        Returns logits: (B, L, V)\n",
        "        \"\"\"\n",
        "        B, L = x_t.shape\n",
        "        tok_emb = self.token_emb(x_t)                        # (B,L,D)\n",
        "        pos_emb = self.pos_emb(self.positions)[None, :, :]   # (1,L,D)\n",
        "        time_emb = self.time_emb(t).unsqueeze(1)             # (B,1,D)\n",
        "\n",
        "        h = tok_emb + pos_emb + time_emb                     # (B,L,D)\n",
        "        logits = self.mlp(h)                                 # (B,L,V)\n",
        "        return logits\n",
        "\n",
        "model = SimpleSEDDModel(vocab_size=vocab_size, d_model=64, T=T, seq_len=seq_len).to(device)\n",
        "\n",
        "# =======================\n",
        "# 4. Score Entropy Loss\n",
        "#    Here: cross-entropy over x_0\n",
        "#    This corresponds to Score Entropy under a delta posterior\n",
        "# =======================\n",
        "\n",
        "def score_entropy_loss(logits, x0):\n",
        "    \"\"\"\n",
        "    logits: (B,L,V)\n",
        "    x0:     (B,L)\n",
        "\n",
        "    Target is one-hot δ(x0).\n",
        "    Loss = - E_{t, x0, x_t} [ log p_theta(x0 | x_t, t) ]\n",
        "    \"\"\"\n",
        "    B, L, V = logits.shape\n",
        "    loss = F.cross_entropy(\n",
        "        logits.view(B * L, V),\n",
        "        x0.view(B * L),\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "\n",
        "# =======================\n",
        "# 5. Training Loop\n",
        "# =======================\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_steps = 2000\n",
        "batch_size = 8\n",
        "\n",
        "def sample_batch():\n",
        "    idx = torch.randint(0, num_samples, (batch_size,), device=device)\n",
        "    x0 = data_x0[idx]                            # (B,L)\n",
        "\n",
        "    # Sample timestep t ~ Uniform{1..T}\n",
        "    t = torch.randint(1, T + 1, (batch_size,), device=device)\n",
        "\n",
        "    # Forward diffusion\n",
        "    x_t = q_sample(x0, t)                        # (B,L)\n",
        "    return x0, x_t, t\n",
        "\n",
        "for step in range(1, num_steps + 1):\n",
        "    model.train()\n",
        "    x0, x_t, t = sample_batch()\n",
        "    logits = model(x_t, t)\n",
        "    loss = score_entropy_loss(logits, x0)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 200 == 0:\n",
        "        print(f\"[Step {step:4d}] loss = {loss.item():.4f}\")\n",
        "\n",
        "        # Visualize denoising from strong noise (t = T)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            demo_idx = random.randrange(num_samples)\n",
        "            clean = data_x0[demo_idx:demo_idx+1]           # (1,L)\n",
        "            t_demo = torch.tensor([T], device=device)      # use maximum noise level\n",
        "            noised = q_sample(clean, t_demo)               # (1,L)\n",
        "            logits_demo = model(noised, t_demo)            # (1,L,V)\n",
        "            pred = logits_demo.argmax(dim=-1)              # (1,L)\n",
        "\n",
        "            def decode(x):\n",
        "                return \" \".join(id2token[int(i)] for i in x)\n",
        "\n",
        "            print(\"  clean: \", decode(clean[0]))\n",
        "            print(\"  noised:\", decode(noised[0]))\n",
        "            print(\"  pred:  \", decode(pred[0]))\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "# =======================\n",
        "# 6. Generation Demo (one-step denoising from pure noise)\n",
        "#    Sample uniform x_T and directly predict x_0\n",
        "# =======================\n",
        "\n",
        "def generate_from_noise(n=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Pure noise: each position sampled uniformly\n",
        "        x_T = torch.randint(0, vocab_size, (n, seq_len), device=device)\n",
        "        t = torch.full((n,), T, device=device, dtype=torch.long)\n",
        "\n",
        "        logits = model(x_T, t)\n",
        "        x0_pred = logits.argmax(dim=-1)\n",
        "\n",
        "        for i in range(n):\n",
        "            print(f\"Sample {i}:\")\n",
        "            print(\"  x_T   :\", \" \".join(id2token[int(x)] for x in x_T[i]))\n",
        "            print(\"  x0hat :\", \" \".join(id2token[int(x)] for x in x0_pred[i]))\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n===== Generation from pure noise (one-step denoise) =====\")\n",
        "generate_from_noise(n=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zev0MWhEY3rV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzE-si49ZaPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}